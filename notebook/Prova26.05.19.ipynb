{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('rslp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlputils import lexical\n",
    "from nlputils import morphosyntax\n",
    "from nlputils import syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chamada da bibioteca de preprocessamento\n",
    "lexical_normalizer = lexical.Preprocessing()\n",
    "morphosyntax_normalizer = morphosyntax.Preprocessing('../models/pt_core_news_sm-2.1.0')\n",
    "syntax_normalizer = syntax.Preprocessing('../models/pt_core_news_sm-2.1.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#definição do diretorio dos corpus e criacao de uma lista com os nomes de cada arquivo dentro do diretorio\n",
    "corpora_path = '../data/corpora/'\n",
    "files_corpora = os.listdir(corpora_path)\n",
    "files_corpora = [d for d in files_corpora if d not in '.DS_Store']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criacao de um dicionario que ira armazenar cada corpus em uma chave\n",
    "sentences_dic = {}\n",
    "all_files = []\n",
    "for corpus in files_corpora:\n",
    "    files = [os.path.join(corpora_path + corpus, f) \\\n",
    "             for f in os.listdir(corpora_path + corpus) \\\n",
    "             if os.path.isfile(os.path.join(corpora_path + corpus, f))]\n",
    "    #cada corpus tera mais 3 chaves para armazenar informacoes de trabalho\n",
    "    sentences_dic[corpus] = {'sentencas': [], 'tag': [], 'parse': [], 'svo': []}\n",
    "    \n",
    "    #adiciona todos os arquivos em uma unica lista independentemente do corpus\n",
    "    print(len(files))\n",
    "    all_files.extend(files)\n",
    "    \n",
    "    #para cada arquivo em um corpus sera extraido suas frases e armazenadas em cada linha de uma lista\n",
    "    for file in files[0:30]:\n",
    "        with open(file, 'r', encoding='utf-8') as text_file:\n",
    "            lines = text_file.readlines()\n",
    "            for line in lines:\n",
    "                if line != '\\n':\n",
    "                    #toda a sentenca sera escrita em letras minusculas\n",
    "                    #line = lexical_normalizer.lowercase(line) \n",
    "                    #tokeniza as sentencas\n",
    "                    sentences_line = lexical_normalizer.tokenize_sentences(line)\n",
    "                    for sentence in sentences_line:\n",
    "                        #print(sentence)\n",
    "                        #adiciona cada sentenca de uma linha no dicionario\n",
    "                        sentences_dic[corpus]['sentencas'].append(sentence)\n",
    "                        #adiciona uma lista de cada palavra da sentenca taggeada composta de \n",
    "                        #(token, etiqueta_morfossintática)\n",
    "                        sentences_dic[corpus]['tag'].append(morphosyntax_normalizer.tag(sentence))\n",
    "                        #adiciona uma lista de cada palavra da sentenca composta de \n",
    "                        #(token, papel_sintático, head),\n",
    "                        sentences_dic[corpus]['parse'].append(syntax_normalizer.parse(sentence))\n",
    "                        #adiciona uma lista de cada sentenca composta por tuplas de (sujeito, verbo, objeto)\n",
    "                        sentences_dic[corpus]['svo'].append(syntax_normalizer.get_SVO(sentence))\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_dic.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utilizacao do Pandas para visualizao dos dados em forma de tabelas\n",
    "import pandas as pd\n",
    "\n",
    "#cracao de um dicionario que ira armazenar cada corpus em suas respectivas keys.\n",
    "dataframes_sentences = {}\n",
    "for key in sentences_dic.keys():\n",
    "    #os corpus armazenados aqui estara em formato de DataFrame onde cada key sera uma coluna da tabela\n",
    "    dataframes_sentences[key] = pd.DataFrame(sentences_dic[key], columns=['sentencas','tag','parse', 'svo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_sentences['animais'].head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_sentences['games'].head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Utilizando\t o\t corpora\t compilado\t para\t a\t Prova\t 1, e\t as\t rotinas\t\n",
    "definidas\t na\t questão\t anterior, realizar\t a\t extração\t de\t informações\t no\t formado\t de\t triplas:\t\n",
    "(Sujeito,\tVerbo,\tObjeto).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Um\tdicionário\tPython\tdeve\tser\tcriado\tda\tseguinte\tforma:\n",
    "- a. {“verbo lematizado1”: [(Sujeito1, Objeto1), (Sujeito2,\n",
    "None), ..., (Sujeiton, Objeton)],\n",
    "...,\n",
    "“verbo lematizadok”: [(Sujeito1, Objeto1), (Sujeito2,\n",
    "Objeto2), ..., (Sujeitom, Objetom)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_verb = {}\n",
    "no_obj_verb = []\n",
    "for corpus in sentences_dic:\n",
    "    for sentence in sentences_dic[corpus]['svo']:\n",
    "        for svo in sentence:\n",
    "            if svo[1] != None:\n",
    "                verb = svo[1].lemma_\n",
    "                if verb in lemma_verb.keys():\n",
    "                    lemma_verb[verb].append((svo[0], svo[2]))\n",
    "                else:\n",
    "                    lemma_verb[verb] = []\n",
    "                    lemma_verb[verb].append((svo[0], svo[2]))\n",
    "                if svo[2] == None and svo[2] not in no_obj_verb:\n",
    "                    no_obj_verb.append((verb, svo[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - b. Exibir\tas\tseguintes\testatísticas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_verbs = {'verbo': [], 'num_svo': []}\n",
    "for verb in lemma_verb:\n",
    "    stat_verbs['verbo'].append(verb)\n",
    "    stat_verbs['num_svo'].append(len(lemma_verb[verb]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - i. Qual\tverbo\ttem\ta\tmaior\tlista\tde\tsujeitos\te\tobjetos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(stat_verbs, columns=['verbo','num_svo'])\n",
    "df.sort_values(by=\"num_svo\").tail(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - ii.Há\talgum\tverbo\tsem\tobjetos?\tMostre\talguns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(no_obj_verb))\n",
    "print(no_obj_verb[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
